{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "linear_beta_vae.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dauparas/tensorflow_examples/blob/master/linear_beta_vae.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Kcg2IoHJWJ8M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Step 1: import dependencies\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from keras import regularizers\n",
        "import time\n",
        "from __future__ import division\n",
        "\n",
        "%matplotlib inline\n",
        "plt.style.use('ggplot')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SFMeLuVmD03N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Step 2: generate some data\n",
        "N_points = 501 #number of points to sample\n",
        "np.random.seed(1) #set random seed\n",
        "\n",
        "#Sample data points from a uniform distribution\n",
        "t1 = np.random.uniform(0, 1, N_points)\n",
        "t2 = np.random.uniform(0, 1, N_points)\n",
        "t3 = np.random.uniform(0, 1, N_points)\n",
        "\n",
        "#Create features and add some Gaussian noise\n",
        "noise_level = 0.01\n",
        "x1 = (1.0*t1+noise_level*np.random.randn(N_points)).reshape(-1,1)\n",
        "x2 = (2.0*t1+noise_level*np.random.randn(N_points)).reshape(-1,1)\n",
        "x3 = (3.0*t2+noise_level*np.random.randn(N_points)).reshape(-1,1)\n",
        "x4 = (5.0*t2+noise_level*np.random.randn(N_points)).reshape(-1,1)\n",
        "N_features = 4\n",
        "X = np.concatenate([x1, x2, x3, x4], axis=1)\n",
        "\n",
        "#Rescale data \n",
        "mean = X.mean(axis=0)\n",
        "std = X.std(axis=0)\n",
        "X = (X - mean) /std\n",
        "X = X.astype(np.float32)\n",
        "assert X.shape == (N_points, N_features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6v-CjMxVr0uf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Plot generating variables\n",
        "plt.scatter(t1,t2);\n",
        "plt.axis('equal');\n",
        "plt.xlabel('t1');\n",
        "plt.ylabel('t2');\n",
        "plt.title('Generating variables');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YMEu-2fhg--c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Plot data points\n",
        "plt.figure(1)\n",
        "plt.scatter(data[:,0], data[:,1]);\n",
        "plt.axis('equal');\n",
        "plt.xlabel('x1');\n",
        "plt.ylabel('x2');\n",
        "plt.title('Data points');\n",
        "plt.axis('equal');\n",
        "#Plot data points\n",
        "plt.figure(2)\n",
        "plt.scatter(data[:,0], data[:,2]);\n",
        "plt.axis('equal');\n",
        "plt.xlabel('x1');\n",
        "plt.ylabel('x3');\n",
        "plt.axis('equal');\n",
        "\n",
        "plt.figure(3)\n",
        "plt.scatter(data[:,2], data[:,3]);\n",
        "plt.axis('equal');\n",
        "plt.xlabel('x3');\n",
        "plt.ylabel('x4');\n",
        "plt.axis('equal');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tXdqEEEvDaRB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Calculate the covariance matrix of the data matrix X\n",
        "C = np.matmul(X.T, X)/X.shape[0]\n",
        "\n",
        "#Plot correlation matrix C\n",
        "f, ax = plt.subplots();\n",
        "sns.heatmap(C, square=True, linewidths=1.0);\n",
        "ax.set_xlabel('features');\n",
        "ax.set_ylabel('features');\n",
        "ax.set_title('Correlation matrix');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "22GHCAkVeolK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Create encoder and decoder for beta-VAE      \n",
        "def encoder(x_in, N_features, N_latent):\n",
        "    with tf.variable_scope(\"encoder\", reuse=None):\n",
        "        w1 = tf.get_variable('w1', [N_features, N_latent], initializer=tf.glorot_uniform_initializer())\n",
        "        b1 = tf.get_variable('b1', [N_latent], initializer=tf.constant_initializer(0.0))\n",
        "        enc_mean = tf.matmul(x_in, w1)+b1 #mean of the encoded Gaussian\n",
        "      \n",
        "      \n",
        "        w2 = tf.get_variable('w2', [N_features, N_latent], initializer=tf.glorot_uniform_initializer())\n",
        "        b2 = tf.get_variable('b2', [N_latent], initializer=tf.constant_initializer(0.0))\n",
        "        enc_log_sd = tf.matmul(x_in, w2)+b2 #log of the standard deviation of the Gaussian\n",
        "        \n",
        "        epsilon = tf.random_normal(tf.shape(enc_mean)) #sample from Gaussian(0,1)\n",
        "        z  = enc_mean + tf.multiply(epsilon, tf.exp(enc_log_sd)) #create the latent sample\n",
        "        return z, enc_mean, enc_log_sd, w1, w2\n",
        "      \n",
        "def decoder(z_in, N_features, N_latent):\n",
        "    with tf.variable_scope(\"decoder\", reuse=tf.AUTO_REUSE):\n",
        "        w3 = tf.get_variable('w3', [N_latent, N_features], initializer=tf.glorot_uniform_initializer())\n",
        "        b3 = tf.get_variable('b3', [N_features], initializer=tf.constant_initializer(0.0))\n",
        "        dec_mean = tf.matmul(z_in, w3)+b3 #mean of the Gaussian output\n",
        "        return dec_mean, w3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WjYvpRXCrfen",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Create latent variables for testing disentanglement\n",
        "N_latent_test = 21\n",
        "z1 = np.linspace(-0.5,0.5,N_latent_test)\n",
        "z2 = np.linspace(-0.5,0.5,N_latent_test) \n",
        "z = np.meshgrid(z1, z2)\n",
        "z = np.reshape(z, [2, -1]).T\n",
        "z = np.asarray(z, dtype=np.float32)\n",
        "assert z.shape == (N_latent_test**2, 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5teAboyfeP_I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Feed data matrix X, connect encoder and decoder, define a loss function\n",
        "tf.reset_default_graph() #reset the tensorflow graph \n",
        "\n",
        "#Parameters for training\n",
        "n_epochs =1000\n",
        "batch_size = N_points\n",
        "N_latent = 2\n",
        "learning_rate = 0.1\n",
        "\n",
        "#Create placeholders for feeding training parameters\n",
        "BATCH_SIZE = tf.placeholder(tf.int64, name='batch') \n",
        "Z = tf.placeholder(tf.float32, name='Z_test') #for inputing latent variables\n",
        "C = tf.placeholder(tf.float32, name='C') #for the capacity of the encoder\n",
        "\n",
        "#Feed data matrix X into tf.data.Dataset\n",
        "train_data = (X, X) #inputs and targets - they are the same for the autoencoder\n",
        "train_data = tf.data.Dataset.from_tensor_slices(train_data)\n",
        "train_data = train_data.shuffle(buffer_size=10000)\n",
        "train_data = train_data.batch(BATCH_SIZE)\n",
        "\n",
        "#Create an iterator\n",
        "iterator = tf.data.Iterator.from_structure(train_data.output_types, train_data.output_shapes)\n",
        "features, labels = iterator.get_next()\n",
        "train_init = iterator.make_initializer(train_data)\n",
        "\n",
        "#Connect encoder and decoder\n",
        "z_sampled, enc_mean, enc_log_sd, w1, w2 = encoder(features, N_features, N_latent)\n",
        "dec_mean, w3 = decoder(z_sampled, N_features, N_latent)\n",
        "\n",
        "#Input latent space testing data\n",
        "dec_mean_out, _ = decoder(Z, N_features, N_latent)\n",
        "\n",
        "\n",
        "#Define losses\n",
        "#Decoder loss - reconstruction loss is a sum of square differences, i.e. we assume that the output distribution\n",
        "#Gaussian(dec_mean, 1)\n",
        "dec_loss = tf.reduce_mean(tf.reduce_sum(tf.square(features-dec_mean),1))\n",
        "\n",
        "#KL loss between the prior p(z) which is Gaussian(0,1) and the decoder which is Gaussian(enc_mean, enc_variance)\n",
        "#see equation (10) in the Kingma, Welling paper (https://arxiv.org/pdf/1312.6114.pdf)\n",
        "lam1 = 0.5\n",
        "kl = tf.reduce_mean(-0.5 * tf.reduce_sum(1.0+ 2.0*enc_log_sd - tf.square(enc_mean) - tf.exp(2.0 * enc_log_sd), 1))\n",
        "kl_loss = lam1*tf.abs(kl-C)\n",
        "\n",
        "#Add L1 regularization on network weights\n",
        "lam2 = 0.1\n",
        "l1_loss = lam2*(tf.reduce_sum(tf.abs(w1))+tf.reduce_sum(tf.abs(w2))+tf.reduce_sum(tf.abs(w3)))\n",
        "\n",
        "#Total loss for training\n",
        "loss = dec_loss+kl_loss+l1_loss\n",
        "\n",
        "#Optimizer for training\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aYQl8NM-dCzY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Create a training session\n",
        "saver = tf.train.Saver()\n",
        "with tf.Session() as sess:\n",
        "    start_time = time.time()\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    for i in range(n_epochs):\n",
        "        c = 5.0*(1.0*i)/1000 #capacity of the encoder\n",
        "        sess.run(train_init, feed_dict={BATCH_SIZE: batch_size, C: c})\n",
        "        total_loss = 0 #total loss\n",
        "        total_kl = 0\n",
        "        total_dec_loss = 0 #total_decoder_loss\n",
        "        total_kl_loss = 0\n",
        "        total_l1_loss = 0\n",
        "        n_batches = 0\n",
        "        try:\n",
        "            while True:\n",
        "                _, l, kl_out, l_dec, l_kl, l_l1= sess.run([optimizer, loss, kl, dec_loss, kl_loss, l1_loss], feed_dict={C: c})\n",
        "                total_loss += l\n",
        "                total_kl += kl_out\n",
        "                total_dec_loss += l_dec\n",
        "                total_kl_loss += l_kl\n",
        "                total_l1_loss += l_l1\n",
        "                n_batches += 1\n",
        "        except tf.errors.OutOfRangeError:\n",
        "            pass\n",
        "        if i % 50 == 0:\n",
        "          print('Epoch: {0}, loss: {1:.4f}, KL: {2:.4f}, dec_loss: {3:.4f}, kl_loss: {4:.4f}, l1_loss: {5:.4f}'\n",
        "                .format(i, total_loss/n_batches, total_kl/n_batches, total_dec_loss/n_batches, \n",
        "                        total_kl_loss/n_batches, total_l1_loss/n_batches ))\n",
        "    print('Total time: {0} seconds'.format(time.time() - start_time))\n",
        "    \n",
        "    \n",
        "    #Calculate the decoder mean for the training data X\n",
        "    sess.run(train_init, feed_dict={BATCH_SIZE: X.shape[0]})\n",
        "    try:\n",
        "      while True:\n",
        "        dec_mean_train = sess.run([dec_mean])\n",
        "    except tf.errors.OutOfRangeError:\n",
        "      pass\n",
        "    \n",
        "    \n",
        "    #Calculate the encoding for the training data X\n",
        "    sess.run(train_init, feed_dict={BATCH_SIZE: X.shape[0]})\n",
        "    try:\n",
        "      while True:\n",
        "        enc_mean_out, enc_log_sd_out = sess.run([enc_mean, enc_log_sd])\n",
        "    except tf.errors.OutOfRangeError:\n",
        "      pass\n",
        "\n",
        "    dec_mean_test = sess.run([dec_mean_out], feed_dict={Z: z})\n",
        "    \n",
        "    \n",
        "    #Save the model\n",
        "    save_path = saver.save(sess, \"./ae.ckpt\")\n",
        "    print(\"Model saved in path: %s\" % save_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1uhCLZoP90db",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "# Create variables to be restored\n",
        "w1 = tf.get_variable('encoder/w1', shape=[N_features, N_latent], dtype=tf.float32)\n",
        "b1 = tf.get_variable('encoder/b1', shape=[N_latent], dtype=tf.float32)\n",
        "w3 = tf.get_variable('decoder/w3', shape=[N_latent,N_features], dtype=tf.float32)\n",
        "b3 = tf.get_variable('decoder/b3', shape=[N_features], dtype=tf.float32)\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "with tf.Session() as sess:\n",
        "  saver.restore(sess, \"./ae.ckpt\")\n",
        "  w1_out = w1.eval()\n",
        "  b1_out = b1.eval()\n",
        "  w3_out = w3.eval()\n",
        "  b3_out = b3.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dERuYfHF66Bz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Plot the encoder matrix w1\n",
        "f, ax = plt.subplots();\n",
        "sns.heatmap(w1_out.T, square=True, linewidths=1.0, vmin=-1.0, vmax=1.0);\n",
        "ax.set_xlabel('features');\n",
        "ax.set_ylabel('latent');\n",
        "ax.set_title('Encoder matrix');\n",
        "\n",
        "#Plot the decoder matrix w3\n",
        "f, ax = plt.subplots();\n",
        "sns.heatmap(w3_out, square=True, linewidths=1.0, vmin=-1.0, vmax=1.0);\n",
        "ax.set_xlabel('features');\n",
        "ax.set_ylabel('latent');\n",
        "ax.set_title('Decoder matrix');\n",
        "\n",
        "#Plot the encoder * decoder matrix\n",
        "f, ax = plt.subplots();\n",
        "sns.heatmap(np.matmul(w1_out, w3_out), linewidths=1.0, vmin=-1.0, vmax=1.0);\n",
        "ax.set_xlabel('features');\n",
        "ax.set_ylabel('features');\n",
        "ax.set_title('Encoder * decoder matrix');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6aJ9Zu-nJp-6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dec_mean_train = np.squeeze(np.array(dec_mean_train))\n",
        "dec_mean_test = np.squeeze(np.array(dec_mean_test))\n",
        "enc_mean_out = np.squeeze(np.array(enc_mean_out))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AAoIiJjHQDrG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.scatter(enc_mean_out[:,0], enc_mean_out[:,1]);\n",
        "plt.axis('equal');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AE95kFX7uioQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dc = np.reshape(dec_mean_test, [N_latent_test, N_latent_test, N_features])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B24XUbmCfTxB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.scatter(X[:,0], X[:,1], label='train_data');\n",
        "# plt.scatter(dec_mean_out[:,0], dec_mean_out[:,1], label='learnt model')\n",
        "plt.scatter(dc[:,0,0], dc[:,0,1], label='changing z1');\n",
        "plt.scatter(dc[0,:,0], dc[0,:,1], label='changing z2');\n",
        "plt.legend();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8GX-uB5i1PXW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.scatter(X[:,2], X[:,3], label='train_data');\n",
        "# plt.scatter(dec_mean_out[:,0], dec_mean_out[:,1], label='learnt model')\n",
        "plt.scatter(dc[:,0,2], dc[:,0,3], label='changing z1');\n",
        "plt.scatter(dc[0,:,2], dc[0,:,3], label='changing z2');\n",
        "plt.legend();"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}